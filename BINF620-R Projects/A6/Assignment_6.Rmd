---
title: "Assignment 6"
author: "Manning Smith"
date: "11/12/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(survival)
library(ranger)
library(ggplot2)
library(ggfortify)
library("survminer")
library(gridExtra)
library(caret)
library(knitr)
library(kableExtra)
library(corrplot)
library(ggcorrplot)
library(mice)
library(class)
library(tidyr)
library(VIM)
library(naivebayes)
library(MASS)
library(boot)
set.seed(43)
```

# Import Data
```{r import, echo=TRUE}
heartData_raw <- read.csv("Heart.csv")
heartData_raw$AHD <- as.factor(heartData_raw$AHD)
heartData_raw <- heartData_raw[, !names(heartData_raw) %in% c("PatientID")]
#View(heartData)
heartData <- na.omit(heartData_raw)
```

## Data Set Information
- PatientID:  unique patient identifier
- Age: age in years
- Sex:  1 = male; 0 = female
- ChestPain: chest pain type
- RestBP:  resting blood pressure (in mm Hg on admission to the hospital)
- Chol: serum cholestoral in mg/dl
- Fbs: fasting blood sugar > 120 mg/dl (1 = true; 0 = false)
- RestECG: resting electrocardiographic results. 0: normal ; 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) ;  2: showing probable or definite left ventricular hypertrophy by Estes' criteria.
- MaxHR: maximum heart rate achieved
- ExAng: exercise induced angina (1 = yes; 0 = no)
- Oldpeak: ST depression induced by exercise relative to rest
- Slope: the slope of the peak exercise ST segment. 1: upsloping; 2: flat; 3: downsloping
- Ca: number of major vessels (0-3) colored by flourosopy
- Thal: Thallium stress test, 3 =normal; 6 = fixed (defect); 7 = reversable (defect)
- AHD (the predicted attribute): angiographic heart disease

\newpage
# Part 1
In nature, with regularization, we apply what is know as a shrinkage penalty in conjunction with RSS (Residual sum of squares) minimization:

$$
\min\left(\sum^n_{i=1}\left(y_i-w_ix_i\right)^2+\:\lambda\left(1-\alpha\right)\sum^n_{i=1}w_i^2+\alpha\sum^n_{i=1}\left|w_i\right|\right)
$$
The penalty consists of $\lambda$ and $\alpha$, tuning parameters, along with the normalization of the coefficients and weights.

Please explain how we can drive (without mathematical proof), from above minimization expression, the OLS based linear regression, Lasso regression, Ridge regression, and elastic net regression, via handling the values of $\lambda$ and $\alpha$.

First term: $\sum^n_{i=1}(y_i-w_ix_i)^2$ is the RSS (Residual Sum of Squares)
Second term: $\lambda(1-\alpha)\sum^n_{i=1}w_i^2$ is the L2 penalty (Ridge)
Third term: $\alpha\sum^n_{i=1}|w_i|$ is the L1 penalty (Lasso)

**OLS based Linear Regression:**\
When $\lambda=0$ then it reduces to $\min(\sum^n_{i=1}(y_i-w_ix_i)^2)$.\

**Ridge Regression:**\
Set $\alpha=0$ and $\lambda>0$ then $\min(\sum^n_{i=1}(y_i-w_ix_i)^2 + \lambda\sum^n_{i=1}w_i^2)$. Only the L2 penalty remains.\

**Lasso Regression:**\
Set $\alpha=1$ and $\lambda>0$ then $\min(\sum^n_{i=1}(y_i-w_ix_i)^2 + \lambda\sum^n_{i=1}|w_i|)$. Only the L1 penalty remains.\

**Elastic Net Regression:**\
Set $0 < \alpha < 1$ and $\lambda>0$ then leave the entire formula with both L1 and L2 penalties.\

*Personal conclusion:*\
- $\lambda$ controls the overall strength of regularization
- $\alpha$ determines the mix between L1 and L2 penalties
- Larger $\lambda$ means stronger regularization
- $\alpha$ closer to 1 means more Lasso-like behavior
- $\alpha$ closer to 0 means more Ridge-like behavior

\newpage
# Part 2
We would like to predict RestBP (resting blood pressure, in mm Hg on admission to the hospital), using regression trees and related approaches, treating the response RestBP (outcome) as a quantitative variable, and other variables are the predictors.

- (a) Split the data set into a training set and a test set.
We will utilize a 70/30 split for our data
```{r p2_splitData, echo=TRUE}
split_index <- createDataPartition(heartData$RestBP, p = 0.7, list = FALSE)
train_data <- heartData[split_index, ]
test_data <- heartData[-split_index, ]
```

- (b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?

**Model:** `RestBP ~ Age + Sex + ChestPain + AHD + Chol + Fbs + RestECG + MaxHR + ExAng + Oldpeak + Slope + Ca + Thal`

```{r p2_regression, echo=TRUE}
tree_model <- rpart(RestBP ~ Age + Sex + ChestPain + AHD + Chol + Fbs + RestECG + MaxHR + ExAng + 
                        Oldpeak + Slope + Ca + Thal,
                   data = train_data,
                   method = "anova")

rpart.plot(tree_model, box.palette = "RdBu", shadow.col = "gray", nn = TRUE)

# Calc MSE
tree_pred <- predict(tree_model, test_data)
tree_mse <- mean((test_data$RestBP - tree_pred)^2)
#tree_model
```

This regression tree provided a large MSE doesn't suggest much strength of evidence. **MSE:** `r round(tree_mse, 3)`

- (c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
```{r p2_crossVal, echo=TRUE}
cv_model <- train(RestBP ~ Age + Sex + ChestPain + AHD + Chol + Fbs + RestECG + MaxHR + ExAng + 
                        Oldpeak + Slope + Ca + Thal, data = train_data, method = "rpart", trControl = trainControl(method = "cv", number = 13))

best_cv <- cv_model$bestTune$cp

# Fit the regression tree with the best cp value
optimal_tree <- rpart(RestBP ~ Age + Sex + ChestPain + AHD + Chol + Fbs + RestECG + MaxHR + ExAng + 
                        Oldpeak + Slope + Ca + Thal, data = train_data, method = "anova", control = rpart.control(cp = best_cv))

# Make predictions on the test set with the pruned tree
predictions_pruned <- predict(optimal_tree, test_data)

# Calculate the MSE for the pruned tree
mse_pruned <- mean((predictions_pruned - test_data$RestBP)^2)
```
The best cv value was `r round(mse_pruned, 4)`.\

The results from the pruning tree are much better than the previous regression tree, but still not ideal to suggest statistical evidence.\
**Pruning Tree MSE:** `r round(mse_pruned, 3)`

The margin of error with the pruned MSE is $\pm$`r round(sqrt(mse_pruned), 3)` mmHg.

- (d) Use the bagging approach (we will cover next week) in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.
```{r p2_bagging, echo=TRUE}
bag_model <- randomForest(RestBP ~ Age + Sex + ChestPain + AHD + Chol + Fbs + RestECG + MaxHR + ExAng + 
                        Oldpeak + Slope + Ca + Thal,
                         data = train_data,
                         mtry = ncol(train_data) - 1,  # use all predictors
                         importance = TRUE)

bag_pred <- predict(bag_model, test_data)
bag_mse <- mean((test_data$RestBP - bag_pred)^2)

# Plot
varImpPlot(bag_model)
```


- (e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.
```{r p2_randomFor, echo=TRUE}
# Fit a random forest model
rf_model <- randomForest(RestBP ~ Age + Sex + ChestPain + AHD + Chol + Fbs + RestECG + MaxHR + ExAng + 
                        Oldpeak + Slope + Ca + Thal, data = train_data, importance = TRUE)

# Make predictions on the test set
rf_predictions <- predict(rf_model, test_data)

# Calculate the test MSE for random forests
rf_mse <- mean((rf_predictions - test_data$RestBP)^2)

# Show variable importance
rf_model$importance

# Effect of m (number of variables at each split) on error rate
rf_model$mtry
```

**rf model MSE:** `r round(rf_mse, 3)`

- (f) Report Complexity Parameter ($c_p$)s in decision tree, the best $c_p$, value which is the one that minimize the prediction error RMSE (root mean squared error), and plot the final tree model.
```{r p2_cp, echo=TRUE}
# Show the cp values for the optimal model
optimal_tree$cptable

# Find the best cp value that minimizes prediction error (RMSE)
best_cp <- optimal_tree$cptable[which.min(optimal_tree$cptable[, "xerror"]), "CP"]

# Plot the final pruned tree
final_tree <- rpart(RestBP ~ Age + Sex + ChestPain + AHD + Chol + Fbs + RestECG + MaxHR + ExAng + 
                        Oldpeak + Slope + Ca + Thal, data = train_data, method = "anova", control = rpart.control(cp = best_cp))

#rpart.plot(final_tree, type = 3, extra = 1)

final_predictions <- predict(final_tree, test_data)
final_mse <- mean((test_data$RestBP - final_predictions)^2)
```

**Best CP:** `r round(best_cp,4)`

```{r p2_combine, echo=FALSE}
# Initialize an empty data frame to store results
results <- data.frame(
  Model = character(),
  Test_MSE = numeric(),
  CP_Value = numeric(),
  stringsAsFactors = FALSE
)

# Add regression tree results to the data frame
results <- rbind(results, data.frame(
  Model = "Regression Tree",
  Test_MSE = tree_mse,
  CP_Value = NA
))

# Add pruned tree results to the data frame
results <- rbind(results, data.frame(
  Model = "Pruned Tree",
  Test_MSE = mse_pruned,
  CP_Value = best_cv
))

# Add bagging results to the data frame
results <- rbind(results, data.frame(
  Model = "Bagging",
  Test_MSE = bag_mse,
  CP_Value = NA
))

# Add random forest results to the data frame
results <- rbind(results, data.frame(
  Model = "Final Tree",
  Test_MSE = final_mse,
  CP_Value = best_cp
))

# Print the final results data frame
kable(results, caption = "Summary of Results") %>%
  kable_styling(full_width = FALSE)
```

#Part 3

Using AHD (the predicted attribute): angiographic heart disease, as outcomes, and other variables are the predictors.

- (a) Split the dataset into training and testing data-sets with appropriate split percentage (you did this in Assignment #2).
We will utilize a 70/30 split for our data
```{r p3_splitData, echo=TRUE}
split_index <- createDataPartition(heartData$RestBP, p = 0.7, list = FALSE)
train_data <- heartData[split_index, ]
test_data <- heartData[-split_index, ]
```

- (b) Create a fully grown tree showing all predictor variables in the training data set, plot the tree structure, and evaluate its performance on the test data. The predict() function can be used for this purpose. Report your model accuracy rate on test data.

**Model:** `AHD ~ Age + Sex + ChestPain + RestBP + Chol + Fbs + RestECG + MaxHR + ExAng + Oldpeak + Slope + Ca + Thal`

```{r p3_tree, echo=TRUE}
full_tree <- rpart(AHD ~ Age + Sex + ChestPain + RestBP + Chol + Fbs + RestECG + MaxHR + ExAng + 
                        Oldpeak + Slope + Ca + Thal,
                  data = train_data,
                  method = "class",
                  control = rpart.control(cp = 0.005))

# Plot full tree
rpart.plot(full_tree, box.palette = "RdBu", shadow.col = "gray", nn = TRUE)

full_pred <- predict(full_tree, test_data, type = "class")
full_conf_matrix <- confusionMatrix(full_pred, test_data$AHD)
full_conf_matrix
```

The Accuracy was $0.7471$ with a 95% CI of $(0.6425, 0.8342)$. Ths evidence suggest viability in the model and the Kappa of $0.495$ suggests a decent model, but there is still plenty of room for improvement.

- (c) Consider whether pruning the tree might lead to improved results. Using “train” and “ trControl” functions (with “cv”) in R package “rpart” or the function cv.tree() in the R package "tree" performs cross-validation, you can determine the optimal level of tree complexity. Fit the model on the training set with suitable parameters, plot model accuracy vs different values of (complexity parameter), and report the best the optimal level of tree complexity and the corresponding model.
```{r p3_crossVal, echo=TRUE}
ctrl <- trainControl(method = "cv", 
                    number = 10,
                    classProbs = TRUE)

# Train model with cross-validation
cv_tree <- train(AHD ~ Age + Sex + ChestPain + RestBP + Chol + Fbs + RestECG + MaxHR + ExAng + 
                        Oldpeak + Slope + Ca + Thal,
                data = train_data,
                method = "rpart",
                trControl = ctrl,
                tuneLength = 10)

# Plot accuracy vs complexity parameter
plot(cv_tree)
```

The best accuracy was $0.773$ at a complexity of $0.053$.

- (d) Identify and plot the final tree model, make predictions on the Test Data, and report model accuracy rate on test data.
```{r p3_treeMod, echo=TRUE}
final_tree <- prune(full_tree, cp = cv_tree$bestTune$cp)

# Plot final tree
rpart.plot(final_tree, box.palette = "RdBu", shadow.col = "gray", nn = TRUE)

# Evaluate final tree on test data
final_pred <- predict(final_tree, test_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, test_data$AHD)
final_conf_matrix
```

- (e) Using the best value for the complexity parameter ($c_p$) you obtained, compare the full fully grown tree model and your final tree model, and make your conclusion about which model would be better.
```{r p3_complexity, echo=TRUE}
comparison <- data.frame(
  Model = c("Full Tree", "Pruned Tree"),
  Accuracy = c(full_conf_matrix$overall["Accuracy"],
               final_conf_matrix$overall["Accuracy"]),
  Sensitivity = c(full_conf_matrix$byClass["Sensitivity"],
                 final_conf_matrix$byClass["Sensitivity"]),
  Specificity = c(full_conf_matrix$byClass["Specificity"],
                 final_conf_matrix$byClass["Specificity"])
)

kable(comparison, caption = "Comparison") %>%
  kable_styling(full_width = FALSE)
```

- (f) Build a random forest model, generate a variable importance plot, rank the importance of variables, provide the Gini index for the predictors, and select the best model via variable selection (Please read textbook, An Introduction to Statistical Learning, Chapter 8, Tree-based Methods, especially on pp. 319-321).
```{r p3_ranForest, echo=TRUE}
rf_model <- randomForest(AHD ~ Age + Sex + ChestPain + RestBP + Chol + Fbs + RestECG + MaxHR + ExAng + 
                        Oldpeak + Slope + Ca + Thal,
                        data = train_data,
                        importance = TRUE,
                        ntree = 500)

# Variable importance plot
varImpPlot(rf_model, main = "Variable Importance")

# Get variable importance measures
importance_df <- as.data.frame(rf_model$importance)
importance_df$Variable <- rownames(importance_df)
importance_df <- importance_df[order(-importance_df$MeanDecreaseGini), ]
print("Variable Importance (Gini Index):")
print(importance_df)

# Evaluate random forest on test data
rf_pred <- predict(rf_model, test_data)
rf_conf_matrix <- confusionMatrix(rf_pred, test_data$AHD)
```

- (g) Comments your results from classification tree and random forest model via performance measurements.
```{r p3_final, echo=TRUE}
final_comparison <- data.frame(
  Model = c("Full Tree", "Pruned Tree", "Random Forest"),
  Accuracy = c(full_conf_matrix$overall["Accuracy"],
               final_conf_matrix$overall["Accuracy"],
               rf_conf_matrix$overall["Accuracy"]),
  Sensitivity = c(full_conf_matrix$byClass["Sensitivity"],
                 final_conf_matrix$byClass["Sensitivity"],
                 rf_conf_matrix$byClass["Sensitivity"]),
  Specificity = c(full_conf_matrix$byClass["Specificity"],
                 final_conf_matrix$byClass["Specificity"],
                 rf_conf_matrix$byClass["Specificity"])
)

kable(final_comparison, caption = "Comparison") %>%
  kable_styling(full_width = FALSE)
```

As seen from these results the model that best fit our desired criteria being to detect AHD while not over fitting is the random forest model. 